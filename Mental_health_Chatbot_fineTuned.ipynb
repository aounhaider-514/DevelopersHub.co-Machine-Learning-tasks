{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7104d3d0-b1b3-491d-a1a7-5e391315dd96",
   "metadata": {},
   "source": [
    "## **Mental Health Support Chatbot (Fine-Tuned)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bcb716-9f6c-4fef-b604-9f8a8b373754",
   "metadata": {},
   "source": [
    "### Installing Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d79276-b05a-4429-8c19-9de3642994da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU transformers datasets accelerate peft bitsandbytes torch\n",
    "!pip install -q sentencepiece \n",
    "# Required for Mistral tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cce83a-c08f-4f1b-a78a-64aac8880ca4",
   "metadata": {},
   "source": [
    "### Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbc75aab-e64d-439f-bbbc-71ed908bfa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                          Situation      emotion  \\\n",
      "0           0  I remember going to the fireworks with my best...  sentimental   \n",
      "1           1  I remember going to the fireworks with my best...  sentimental   \n",
      "2           2  I remember going to the fireworks with my best...  sentimental   \n",
      "3           3  I remember going to the fireworks with my best...  sentimental   \n",
      "4           4  I remember going to the fireworks with my best...  sentimental   \n",
      "\n",
      "                                empathetic_dialogues  \\\n",
      "0  Customer :I remember going to see the firework...   \n",
      "1  Customer :This was a best friend. I miss her.\\...   \n",
      "2              Customer :We no longer talk.\\nAgent :   \n",
      "3  Customer :Was this a friend you were in love w...   \n",
      "4             Customer :Where has she gone?\\nAgent :   \n",
      "\n",
      "                                              labels Unnamed: 5 Unnamed: 6  \n",
      "0  Was this a friend you were in love with, or ju...        NaN        NaN  \n",
      "1                                Where has she gone?        NaN        NaN  \n",
      "2  Oh was this something that happened because of...        NaN        NaN  \n",
      "3                This was a best friend. I miss her.        NaN        NaN  \n",
      "4                                 We no longer talk.        NaN        NaN  \n",
      "\n",
      "Dataset shape: (64636, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "# Load the Kaggle CSV\n",
    "df = pd.read_csv(\"C:/Users/Aoun Haider/Downloads/emotion-emotion_69k.csv\")\n",
    "\n",
    "# Display sample to verify\n",
    "print(df.head())\n",
    "print(f\"\\nDataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d5416-7f84-43a8-8c94-f422be060419",
   "metadata": {},
   "source": [
    "### Step 2: Robust Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "814508ad-7e89-4a40-9370-6e46bc542442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_row(row):\n",
    "    \"\"\"Process a single row of the Kaggle dataset\"\"\"\n",
    "    # Extract components\n",
    "    emotion = row['emotion']\n",
    "    dialogue = row['empathetic_dialogues']\n",
    "    \n",
    "    # Clean and split dialogue\n",
    "    dialogue_clean = dialogue.replace('\"\"', '\"').replace('\\\\t', ' ').replace('\\\\n', '\\n').strip()\n",
    "    \n",
    "    # Extract user input and bot response\n",
    "    user_input = \"\"\n",
    "    bot_response = \"\"\n",
    "    \n",
    "    # Case 1: Standard format with Customer/Agent markers\n",
    "    if \"Customer :\" in dialogue_clean and \"Agent :\" in dialogue_clean:\n",
    "        try:\n",
    "            parts = re.split(r'Customer :|Agent :', dialogue_clean)\n",
    "            user_input = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "            bot_response = parts[2].strip() if len(parts) > 2 else \"\"\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Case 2: Fallback if markers missing\n",
    "    if not user_input or not bot_response:\n",
    "        if '\\n' in dialogue_clean:\n",
    "            parts = dialogue_clean.split('\\n')\n",
    "            user_input = parts[0].replace(\"Customer:\", \"\").replace(\"Customer :\", \"\").strip()\n",
    "            bot_response = parts[1].replace(\"Agent:\", \"\").replace(\"Agent :\", \"\").strip() if len(parts) > 1 else \"\"\n",
    "        else:\n",
    "            user_input = dialogue_clean\n",
    "            bot_response = row['labels']  # Use labels column as fallback\n",
    "    \n",
    "    # Final cleaning\n",
    "    user_input = re.sub(r'[^a-zA-Z0-9,.!?\\'\" ]', '', user_input)\n",
    "    bot_response = re.sub(r'[^a-zA-Z0-9,.!?\\'\" ]', '', bot_response)\n",
    "    \n",
    "    return {\n",
    "        \"context\": emotion,\n",
    "        \"user_input\": user_input,\n",
    "        \"bot_response\": bot_response\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_data = []\n",
    "for idx, row in df.iterrows():\n",
    "    processed_data.append(preprocess_row(row))\n",
    "    \n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "\n",
    "# Filter out empty responses\n",
    "processed_df = processed_df[processed_df['bot_response'].str.len() > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ea71d7b-a038-47cb-a925-8f730b799cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Situation</th>\n",
       "      <th>emotion</th>\n",
       "      <th>empathetic_dialogues</th>\n",
       "      <th>labels</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>Customer :I remember going to see the firework...</td>\n",
       "      <td>Was this a friend you were in love with, or ju...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>Customer :This was a best friend. I miss her.\\...</td>\n",
       "      <td>Where has she gone?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>Customer :We no longer talk.\\nAgent :</td>\n",
       "      <td>Oh was this something that happened because of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>Customer :Was this a friend you were in love w...</td>\n",
       "      <td>This was a best friend. I miss her.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>Customer :Where has she gone?\\nAgent :</td>\n",
       "      <td>We no longer talk.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          Situation      emotion  \\\n",
       "0           0  I remember going to the fireworks with my best...  sentimental   \n",
       "1           1  I remember going to the fireworks with my best...  sentimental   \n",
       "2           2  I remember going to the fireworks with my best...  sentimental   \n",
       "3           3  I remember going to the fireworks with my best...  sentimental   \n",
       "4           4  I remember going to the fireworks with my best...  sentimental   \n",
       "\n",
       "                                empathetic_dialogues  \\\n",
       "0  Customer :I remember going to see the firework...   \n",
       "1  Customer :This was a best friend. I miss her.\\...   \n",
       "2              Customer :We no longer talk.\\nAgent :   \n",
       "3  Customer :Was this a friend you were in love w...   \n",
       "4             Customer :Where has she gone?\\nAgent :   \n",
       "\n",
       "                                              labels Unnamed: 5 Unnamed: 6  \n",
       "0  Was this a friend you were in love with, or ju...        NaN        NaN  \n",
       "1                                Where has she gone?        NaN        NaN  \n",
       "2  Oh was this something that happened because of...        NaN        NaN  \n",
       "3                This was a best friend. I miss her.        NaN        NaN  \n",
       "4                                 We no longer talk.        NaN        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e55ed-f2ce-4d84-8690-00a4c5c8faad",
   "metadata": {},
   "source": [
    "### Step 3: Create Training-Ready Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9ebc4ed-8d83-4395-9b92-de4ef3c46b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed sample:\n",
      "CONTEXT:  but what I didn't know was that he was working in the next room with the door open.  He approached and asked what I had been saying.  I knew I was caught.  I was so disgusted with myself.  \n",
      "USER: ashamed\n",
      "BOT: Customer Pretty awful, actually.  Especially considering that the coworker of whom I was speaking was in a room just next door with the door open.  Agent \n",
      "\n",
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['context', 'user_input', 'bot_response', 'text', '__index_level_0__'],\n",
      "        num_rows: 36\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['context', 'user_input', 'bot_response', 'text', '__index_level_0__'],\n",
      "        num_rows: 9\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Create final text format\n",
    "processed_df['text'] = processed_df.apply(\n",
    "    lambda x: f\"CONTEXT: {x['context']}\\nUSER: {x['user_input']}\\nBOT: {x['bot_response']}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Split into train/validation\n",
    "train_df = processed_df.sample(frac=0.8, random_state=42)\n",
    "val_df = processed_df.drop(train_df.index)\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"validation\": Dataset.from_pandas(val_df)\n",
    "})\n",
    "\n",
    "# Verify\n",
    "print(\"\\nProcessed sample:\")\n",
    "print(dataset[\"train\"][\"text\"][0])\n",
    "print(\"\\nDataset structure:\", dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd4873-49d5-4b76-b9a7-6ceaef5b9777",
   "metadata": {},
   "source": [
    "### Step 4: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8fbde0-b55b-4206-a531-1ad67c0b1167",
   "metadata": {},
   "source": [
    "### Updated Tokenization for GPT-Neo-1.3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5739dd-1fbd-4e7b-aad4-2d2278c0303d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74500b8f9144219a06fe26c5c9694ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1941342824ee435fb29d1925a3e0ebc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete for GPT-Neo-1.3B!\n",
      "Sample tokenized output: {'context': \" but what I didn't know was that he was working in the next room with the door open.  He approached and asked what I had been saying.  I knew I was caught.  I was so disgusted with myself.  \", 'user_input': 'ashamed', 'bot_response': 'Customer Pretty awful, actually.  Especially considering that the coworker of whom I was speaking was in a room just next door with the door open.  Agent ', 'text': \"CONTEXT:  but what I didn't know was that he was working in the next room with the door open.  He approached and asked what I had been saying.  I knew I was caught.  I was so disgusted with myself.  \\nUSER: ashamed\\nBOT: Customer Pretty awful, actually.  Especially considering that the coworker of whom I was speaking was in a room just next door with the door open.  Agent \", '__index_level_0__': 53779, 'input_ids': [10943, 32541, 25, 220, 475, 644, 314, 1422, 470, 760, 373, 326, 339, 373, 1762, 287, 262, 1306, 2119, 351, 262, 3420, 1280, 13, 220, 679, 10448, 290, 1965, 644, 314, 550, 587, 2282, 13, 220, 314, 2993, 314, 373, 4978, 13, 220, 314, 373, 523, 36943, 351, 3589, 13, 220, 220, 198, 29904, 25, 22461, 198, 33, 2394, 25, 22092, 20090, 12659, 11, 1682, 13, 220, 18948, 6402, 326, 262, 30521, 263, 286, 4150, 314, 373, 5486, 373, 287, 257, 2119, 655, 1306, 3420, 351, 262, 3420, 1280, 13, 220, 15906, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use the smaller, CPU-friendly model\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token (crucial for training)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text with truncation and padding\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",  # Pad all sequences to same length\n",
    "        return_tensors=\"pt\"    # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Re-tokenize datasets\n",
    "tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_val = dataset[\"validation\"].map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Tokenization complete for GPT-Neo-1.3B!\")\n",
    "print(f\"Sample tokenized output: {tokenized_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8af25c2-038e-45fa-bab0-b3f95d425798",
   "metadata": {},
   "source": [
    "### Step 5: Configure LoRA (Low-Rank Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b0351f-8d63-48df-a06b-a5838e2f5132",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: bitsandbytes 0.46.0\n",
      "Uninstalling bitsandbytes-0.46.0:\n",
      "  Successfully uninstalled bitsandbytes-0.46.0\n",
      "Collecting bitsandbytes==0.41.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: HTTP error 404 while getting https://github.com/jllllll/bitsandbytes-windows-webui/raw/main/bitsandbytes-0.41.1-py3-none-win_amd64.whl\n",
      "ERROR: Could not install requirement bitsandbytes==0.41.1 from https://github.com/jllllll/bitsandbytes-windows-webui/raw/main/bitsandbytes-0.41.1-py3-none-win_amd64.whl because of HTTP error 404 Client Error: Not Found for url: https://github.com/jllllll/bitsandbytes-windows-webui/raw/main/bitsandbytes-0.41.1-py3-none-win_amd64.whl for URL https://github.com/jllllll/bitsandbytes-windows-webui/raw/main/bitsandbytes-0.41.1-py3-none-win_amd64.whl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (from scipy) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "# Uninstall existing bitsandbytes\n",
    "!pip uninstall -y bitsandbytes\n",
    "\n",
    "# Install Windows-compatible version from unofficial build\n",
    "!pip install https://github.com/jllllll/bitsandbytes-windows-webui/raw/main/bitsandbytes-0.41.1-py3-none-win_amd64.whl\n",
    "\n",
    "# Install required dependencies\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be1713d3-c11e-4440-83ba-b5b906e11902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_xet in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (1.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "619060d1-3237-4ac6-8590-295155faed97",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (0.33.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (1.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aoun haider\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81422501-7777-49a9-a9f0-9c1ee70e35f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "### **Step 2: Load Model and Tokenizer**\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Tiny model for fast training\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Critical!\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77a08971-7b65-421c-b192-c296a8ca3bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 73,728 || all params: 81,986,304 || trainable%: 0.0899\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "### **Step 3: LoRA Setup**\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Minimal LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                # Tiny rank for speed\n",
    "    target_modules=[\"c_attn\"],  # Only attention weights\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "# Output should show ~0.18% of parameters trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470a6cf5-7a03-479d-9a06-c0f3a52cfebc",
   "metadata": {},
   "source": [
    "## Training setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7e277fd-f6d5-473a-b002-304346767c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2360ce02-c306-4d1d-8822-5258746fa8dc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 3.9.2\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "### **Step 2: Verify Installation**\n",
    "import keras\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "# Should output 2.13.x or similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c34156b-e700-42f6-b1df-23c4799760e9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Data collator\u001b[39;00m\n\u001b[0;32m      4\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2045\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   2044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2045\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   2046\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2075\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2073\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2075\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2073\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2072\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2073\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2075\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:42\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Optional, Union\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     43\u001b[0m     get_reporting_integration_callbacks,\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# isort: on\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhf_hub_utils\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2045\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   2044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2045\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   2046\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2075\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2073\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2075\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2073\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2072\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2073\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2075\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TFPreTrainedModel, TrainingArguments\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     40\u001b[0m     PushToHubMixin,\n\u001b[0;32m     41\u001b[0m     flatten_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     logging,\n\u001b[0;32m     47\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2045\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   2044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2045\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   2046\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2075\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2073\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2075\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2073\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2072\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2073\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2075\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\activations_tf.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m         )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n\u001b[0;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    https://arxiv.org/abs/1606.08415\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Fast training config\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./quick_empathy_train\",\n",
    "    per_device_train_batch_size=8,      # Larger batch since model is small\n",
    "    num_train_epochs=2,                 # Fewer epochs\n",
    "    learning_rate=1e-4,                 # Slightly higher rate\n",
    "    logging_steps=10,                   # More frequent updates\n",
    "    save_strategy=\"no\",                 # Don't save checkpoints\n",
    "    report_to=\"none\"                    # No external logging\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec7db2-934e-47b7-8c98-aeacb641f232",
   "metadata": {},
   "source": [
    "### Doing Training using Pytorch as Keras as some model issues while training ( it trains on older model etc ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e79c4c2-5460-43bf-be7b-2eb79ba08800",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: keras 3.9.2\n",
      "Uninstalling keras-3.9.2:\n",
      "  Successfully uninstalled keras-3.9.2\n",
      "Found existing installation: tensorflow 2.19.0\n",
      "Uninstalling tensorflow-2.19.0:\n",
      "  Successfully uninstalled tensorflow-2.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping keras-nightly as it is not installed.\n",
      "WARNING: Skipping keras-preprocessing as it is not installed.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Aoun Haider\\anaconda3\\Lib\\site-packages\\~-nsorflow'.\n",
      "You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "# First uninstall problematic Keras versions\n",
    "!pip uninstall -y keras keras-nightly keras-preprocessing tensorflow\n",
    "\n",
    "# Install only what we need\n",
    "!pip install -qU transformers datasets peft torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "390197b8-9b52-4464-8a38-a33f8a050c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "### **Fixed Step 4: Prepare Data Loader**\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # For causal language modeling\n",
    ")\n",
    "\n",
    "# Create PyTorch Dataset\n",
    "class EmpathyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encoded_data):\n",
    "        self.encodings = {\n",
    "            \"input_ids\": encoded_data[\"input_ids\"],\n",
    "            \"attention_mask\": encoded_data[\"attention_mask\"]\n",
    "        }\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.encodings[\"input_ids\"][idx]),\n",
    "            \"attention_mask\": torch.tensor(self.encodings[\"attention_mask\"][idx])\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EmpathyDataset(tokenized_train)\n",
    "val_dataset = EmpathyDataset(tokenized_val)\n",
    "\n",
    "# Create data loader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c88732da-5ad1-4041-adff-0f7b1608e170",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.transformer.wte.weight: requires_grad=False\n",
      "base_model.model.transformer.wpe.weight: requires_grad=False\n",
      "base_model.model.transformer.h.0.ln_1.weight: requires_grad=False\n",
      "base_model.model.transformer.h.0.ln_1.bias: requires_grad=False\n",
      "base_model.model.transformer.h.0.attn.c_attn.weight: requires_grad=False\n",
      "base_model.model.transformer.h.0.attn.c_attn.bias: requires_grad=False\n",
      "base_model.model.transformer.h.0.attn.c_proj.weight: requires_grad=False\n",
      "base_model.model.transformer.h.0.attn.c_proj.bias: requires_grad=False\n",
      "base_model.model.transformer.h.0.ln_2.weight: requires_grad=False\n",
      "base_model.model.transformer.h.0.ln_2.bias: requires_grad=False\n",
      "base_model.model.transformer.h.0.mlp.c_fc.weight: requires_grad=False\n",
      "base_model.model.transformer.h.0.mlp.c_fc.bias: requires_grad=False\n",
      "base_model.model.transformer.h.0.mlp.c_proj.weight: requires_grad=False\n",
      "base_model.model.transformer.h.0.mlp.c_proj.bias: requires_grad=False\n",
      "base_model.model.transformer.h.1.ln_1.weight: requires_grad=False\n",
      "base_model.model.transformer.h.1.ln_1.bias: requires_grad=False\n",
      "base_model.model.transformer.h.1.attn.c_attn.weight: requires_grad=False\n",
      "base_model.model.transformer.h.1.attn.c_attn.bias: requires_grad=False\n",
      "base_model.model.transformer.h.1.attn.c_proj.weight: requires_grad=False\n",
      "base_model.model.transformer.h.1.attn.c_proj.bias: requires_grad=False\n",
      "base_model.model.transformer.h.1.ln_2.weight: requires_grad=False\n",
      "base_model.model.transformer.h.1.ln_2.bias: requires_grad=False\n",
      "base_model.model.transformer.h.1.mlp.c_fc.weight: requires_grad=False\n",
      "base_model.model.transformer.h.1.mlp.c_fc.bias: requires_grad=False\n",
      "base_model.model.transformer.h.1.mlp.c_proj.weight: requires_grad=False\n",
      "base_model.model.transformer.h.1.mlp.c_proj.bias: requires_grad=False\n",
      "base_model.model.transformer.h.2.ln_1.weight: requires_grad=False\n",
      "base_model.model.transformer.h.2.ln_1.bias: requires_grad=False\n",
      "base_model.model.transformer.h.2.attn.c_attn.weight: requires_grad=False\n",
      "base_model.model.transformer.h.2.attn.c_attn.bias: requires_grad=False\n",
      "base_model.model.transformer.h.2.attn.c_proj.weight: requires_grad=False\n",
      "base_model.model.transformer.h.2.attn.c_proj.bias: requires_grad=False\n",
      "base_model.model.transformer.h.2.ln_2.weight: requires_grad=False\n",
      "base_model.model.transformer.h.2.ln_2.bias: requires_grad=False\n",
      "base_model.model.transformer.h.2.mlp.c_fc.weight: requires_grad=False\n",
      "base_model.model.transformer.h.2.mlp.c_fc.bias: requires_grad=False\n",
      "base_model.model.transformer.h.2.mlp.c_proj.weight: requires_grad=False\n",
      "base_model.model.transformer.h.2.mlp.c_proj.bias: requires_grad=False\n",
      "base_model.model.transformer.h.3.ln_1.weight: requires_grad=False\n",
      "base_model.model.transformer.h.3.ln_1.bias: requires_grad=False\n",
      "base_model.model.transformer.h.3.attn.c_attn.weight: requires_grad=False\n",
      "base_model.model.transformer.h.3.attn.c_attn.bias: requires_grad=False\n",
      "base_model.model.transformer.h.3.attn.c_proj.weight: requires_grad=False\n",
      "base_model.model.transformer.h.3.attn.c_proj.bias: requires_grad=False\n",
      "base_model.model.transformer.h.3.ln_2.weight: requires_grad=False\n",
      "base_model.model.transformer.h.3.ln_2.bias: requires_grad=False\n",
      "base_model.model.transformer.h.3.mlp.c_fc.weight: requires_grad=False\n",
      "base_model.model.transformer.h.3.mlp.c_fc.bias: requires_grad=False\n",
      "base_model.model.transformer.h.3.mlp.c_proj.weight: requires_grad=False\n",
      "base_model.model.transformer.h.3.mlp.c_proj.bias: requires_grad=False\n",
      "base_model.model.transformer.h.4.ln_1.weight: requires_grad=False\n",
      "base_model.model.transformer.h.4.ln_1.bias: requires_grad=False\n",
      "base_model.model.transformer.h.4.attn.c_attn.weight: requires_grad=False\n",
      "base_model.model.transformer.h.4.attn.c_attn.bias: requires_grad=False\n",
      "base_model.model.transformer.h.4.attn.c_proj.weight: requires_grad=False\n",
      "base_model.model.transformer.h.4.attn.c_proj.bias: requires_grad=False\n",
      "base_model.model.transformer.h.4.ln_2.weight: requires_grad=False\n",
      "base_model.model.transformer.h.4.ln_2.bias: requires_grad=False\n",
      "base_model.model.transformer.h.4.mlp.c_fc.weight: requires_grad=False\n",
      "base_model.model.transformer.h.4.mlp.c_fc.bias: requires_grad=False\n",
      "base_model.model.transformer.h.4.mlp.c_proj.weight: requires_grad=False\n",
      "base_model.model.transformer.h.4.mlp.c_proj.bias: requires_grad=False\n",
      "base_model.model.transformer.h.5.ln_1.weight: requires_grad=False\n",
      "base_model.model.transformer.h.5.ln_1.bias: requires_grad=False\n",
      "base_model.model.transformer.h.5.attn.c_attn.weight: requires_grad=False\n",
      "base_model.model.transformer.h.5.attn.c_attn.bias: requires_grad=False\n",
      "base_model.model.transformer.h.5.attn.c_proj.weight: requires_grad=False\n",
      "base_model.model.transformer.h.5.attn.c_proj.bias: requires_grad=False\n",
      "base_model.model.transformer.h.5.ln_2.weight: requires_grad=False\n",
      "base_model.model.transformer.h.5.ln_2.bias: requires_grad=False\n",
      "base_model.model.transformer.h.5.mlp.c_fc.weight: requires_grad=False\n",
      "base_model.model.transformer.h.5.mlp.c_fc.bias: requires_grad=False\n",
      "base_model.model.transformer.h.5.mlp.c_proj.weight: requires_grad=False\n",
      "base_model.model.transformer.h.5.mlp.c_proj.bias: requires_grad=False\n",
      "base_model.model.transformer.ln_f.weight: requires_grad=False\n",
      "base_model.model.transformer.ln_f.bias: requires_grad=False\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "### **Step 1: Verify Model Setup**\n",
    "# Check if model parameters require gradients\n",
    "for name, param in peft_model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")\n",
    "    \n",
    "# Should show True for LoRA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a32dfc1-d779-4afd-bfd6-cfcd2469a763",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|          | 0/5 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    650\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[0;32m    354\u001b[0m     tensors,\n\u001b[0;32m    355\u001b[0m     grad_tensors_,\n\u001b[0;32m    356\u001b[0m     retain_graph,\n\u001b[0;32m    357\u001b[0m     create_graph,\n\u001b[0;32m    358\u001b[0m     inputs,\n\u001b[0;32m    359\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    360\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "peft_model = peft_model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(peft_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    peft_model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    # Progress bar\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = peft_model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Update progress\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        avg_loss = total_loss / batch_count\n",
    "        progress_bar.set_postfix({\"avg_loss\": f\"{avg_loss:.4f}\"})\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} complete! Average loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Save adapter\n",
    "peft_model.save_pretrained(\"empathy_adapter\")\n",
    "print(\"Training complete! Adapter saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f19bc6a-4918-40ed-8a73-7bc0b060f9ae",
   "metadata": {},
   "source": [
    "## updating code as with 2 epochs model was not working correctly ( and changing epochs was givimng error also ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a63e046f-5c9a-44ce-9520-1deca72fbb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping keras as it is not installed.\n",
      "WARNING: Skipping keras-nightly as it is not installed.\n",
      "WARNING: Skipping keras-preprocessing as it is not installed.\n",
      "WARNING: Skipping tensorflow as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "### **Step 1: Install Essentials (No Keras)**\n",
    "!pip uninstall -y keras keras-nightly keras-preprocessing tensorflow\n",
    "!pip install -qU transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d932edcd-0b78-497f-8f72-0331ba976ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "### **Step 2: Load Model & Tokenizer**\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Critical!\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7a4bfa0-eea4-42c7-a1be-49c911c2248a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataLoader with 5 batches\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "### **Fixed Step 3: Prepare Data Loader**\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        \"\"\"\n",
    "        Creates a PyTorch Dataset from Hugging Face Dataset\n",
    "        with input_ids and attention_mask\n",
    "        \"\"\"\n",
    "        self.input_ids = hf_dataset[\"input_ids\"]\n",
    "        self.attention_mask = hf_dataset[\"attention_mask\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[idx]),\n",
    "            \"attention_mask\": torch.tensor(self.attention_mask[idx])\n",
    "        }\n",
    "\n",
    "# Create PyTorch Dataset\n",
    "train_dataset = ChatDataset(tokenized_train)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "print(f\"Created DataLoader with {len(train_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfbe3e8-636b-4d6a-bf31-d05aa7f8d075",
   "metadata": {},
   "source": [
    "## trying to achieve average loss < 4 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c08078ad-1f3f-44d2-a1ba-3aa4cb50b84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|| 5/5 [01:03<00:00, 12.75s/it, avg_loss=4.6185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete! Average loss: 4.6185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|| 5/5 [01:02<00:00, 12.55s/it, avg_loss=1.1874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 complete! Average loss: 1.1874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|| 5/5 [01:01<00:00, 12.34s/it, avg_loss=1.0866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 complete! Average loss: 1.0866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|| 5/5 [01:03<00:00, 12.72s/it, avg_loss=0.9331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 complete! Average loss: 0.9331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|| 5/5 [01:03<00:00, 12.68s/it, avg_loss=0.8637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 complete! Average loss: 0.8637\n",
      "Training complete! Model saved.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "### **Step 4: Robust Training Loop**\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=input_ids  # Use input_ids as labels for causal LM\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        avg_loss = total_loss / batch_count\n",
    "        progress_bar.set_postfix({\"avg_loss\": f\"{avg_loss:.4f}\"})\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} complete! Average loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"empathy_chatbot\")\n",
    "tokenizer.save_pretrained(\"empathy_chatbot\")\n",
    "print(\"Training complete! Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b333b-8f94-45fc-9d67-f5ddfb14eac9",
   "metadata": {},
   "source": [
    "## Testing Model on some example cases/ dialogues ( while first loading model ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d1d42c4-bf58-4476-b63b-d07057f60830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "### **Step 1: Load Your Trained Model**\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load your custom-trained model\n",
    "model_path = \"empathy_chatbot\"  # Path where you saved the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "print(\"Trained model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a1808a4-b6af-4a53-836b-d96e56551c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add at the top\n",
    "import random\n",
    "\n",
    "def rule_based_fallback(user_input):\n",
    "    \"\"\"Simple empathetic responses\"\"\"\n",
    "    user_input = user_input.lower()\n",
    "    responses = [\n",
    "        \"That sounds really difficult. Would you like to talk more about it?\",\n",
    "        \"I hear you. That must be challenging.\",\n",
    "        \"Thank you for sharing. I'm here to listen.\",\n",
    "        \"I understand this is tough. Remember to be kind to yourself.\",\n",
    "        \"That sounds stressful. Maybe try taking some deep breaths?\"\n",
    "    ]\n",
    "    return random.choice(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97825ade-8363-48f5-b33c-90eac4568a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Refined Mental Health Support Chatbot\n",
      "Type 'exit' to quit\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  I've been feeling isolated lately\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I hear you. Would you like to share more about how you're feeling?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:   I feel tired after coding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Coding fatigue is real! Try the 20-20-20 rule: every 20 minutes, look at something 20 feet away for 20 seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  I'm feeling overwhelmed with work deadlines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: My last day was when the first lady and my wife were in our room, sitting next to each other. When feeling overwhelmed, try breaking tasks into smaller steps.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  I can't sleep because my mind won't stop racing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: It's like the other day. A warm drink and deep breathing before bed might help calm your mind.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  ok thanks , quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: if the user is not a regular person, he has to go around his door and see what it says.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Thank you for chatting. Your mental health matters!\n"
     ]
    }
   ],
   "source": [
    "def enhance_response(user_input, raw_response):\n",
    "    user_input_lower = user_input.lower()\n",
    "    raw_response = raw_response.strip()\n",
    "    \n",
    "    # 1. Handle empty responses\n",
    "    if not raw_response or len(raw_response.split()) < 3:\n",
    "        return handle_empty_response(user_input_lower)\n",
    "    \n",
    "    # 2. Fix contradictory phrases\n",
    "    if \"lonely\" in user_input_lower or \"isolated\" in user_input_lower:\n",
    "        if \"great experience\" in raw_response:\n",
    "            return \"Loneliness can be painful. Would you like to talk more about what you're experiencing?\"\n",
    "    \n",
    "    # 3. Situation-specific enhancements\n",
    "    enhancement = \"\"\n",
    "    if \"overwhelm\" in user_input_lower or \"stress\" in user_input_lower:\n",
    "        enhancement = \" When feeling overwhelmed, try breaking tasks into smaller steps.\"\n",
    "    elif \"tired\" in user_input_lower or \"exhaust\" in user_input_lower:\n",
    "        enhancement = \" Remember to take regular breaks and stay hydrated while working.\"\n",
    "    elif \"sleep\" in user_input_lower or \"racing\" in user_input_lower:\n",
    "        enhancement = \" A warm drink and deep breathing before bed might help calm your mind.\"\n",
    "    \n",
    "    # 4. Ensure proper sentence structure\n",
    "    if not raw_response.endswith(('.', '!', '?')):\n",
    "        raw_response += '.'\n",
    "    \n",
    "    return raw_response + enhancement\n",
    "\n",
    "def handle_empty_response(user_input):\n",
    "    \"\"\"Provide empathetic responses when model returns empty\"\"\"\n",
    "    if \"tired\" in user_input or \"exhaust\" in user_input:\n",
    "        return \"Coding fatigue is real! Try the 20-20-20 rule: every 20 minutes, look at something 20 feet away for 20 seconds.\"\n",
    "    elif \"work\" in user_input or \"deadline\" in user_input:\n",
    "        return \"Work pressure can be draining. Remember to prioritize tasks and take short breaks.\"\n",
    "    else:\n",
    "        return \"I hear you. Would you like to share more about how you're feeling?\"\n",
    "\n",
    "# Updated chat interface with better error handling\n",
    "def mental_health_chat():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Refined Mental Health Support Chatbot\")\n",
    "    print(\"Type 'exit' to quit\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nYou: \")\n",
    "            if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "                print(\"Bot: Thank you for chatting. Your mental health matters!\")\n",
    "                break\n",
    "                \n",
    "            # Generate response\n",
    "            prompt = f\"USER: {user_input}\\nBOT:\"\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=80,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.3,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            raw_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract bot's response\n",
    "            if \"BOT:\" in raw_response:\n",
    "                raw_response = raw_response.split(\"BOT:\")[-1].strip()\n",
    "            else:\n",
    "                raw_response = raw_response.split(\"USER:\")[0].strip()\n",
    "            \n",
    "            # Enhance and print\n",
    "            final_response = enhance_response(user_input, raw_response)\n",
    "            print(f\"Bot: {final_response}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Bot: I encountered an error. Please try rephrasing: {str(e)}\")\n",
    "\n",
    "# Start the chat\n",
    "mental_health_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc3a187-e4f9-4102-b92c-0b3c4d72611c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
